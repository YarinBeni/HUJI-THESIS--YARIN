#!/usr/bin/env python3
"""Create masked restoration dataset from eBL fragments."""
from __future__ import annotations

import argparse
import json
import random
from pathlib import Path

from datasets import Dataset, DatasetDict
import numpy as np

from common_atf import strip_atf

MAX_LEN = 768
MASK_RATIO = (0.15, 0.25)
SPAN_PROB = 0.30
SPAN_LEN = (2, 10)
PAD_TOKEN = ord(" ")


def mask_sequence(seq: list[int], valid_len: int, rng: random.Random) -> tuple[list[int], list[int]]:
    tokens = seq[:]
    labels = [-100] * len(tokens)
    total = valid_len
    target_masks = max(1, int(total * rng.uniform(*MASK_RATIO)))
    masked = 0
    attempts = 0
    while masked < target_masks and attempts < total * 3:
        attempts += 1
        if rng.random() < SPAN_PROB:
            span = rng.randint(*SPAN_LEN)
            start = rng.randint(0, max(0, total - span))
            end = min(total, start + span)
            for i in range(start, end):
                if labels[i] != -100:
                    continue
                labels[i] = tokens[i]
                tokens[i] = ord('#')
                masked += 1
        else:
            idx = rng.randint(0, total - 1)
            if labels[idx] == -100:
                labels[idx] = tokens[idx]
                tokens[idx] = ord('-')
                masked += 1
    return tokens, labels


def process_fragments(fragments, rng):
    input_ids = []
    labels = []
    attention_masks = []
    for frag in fragments:
        atf = frag.get("atf")
        cleaned = strip_atf(atf)
        if not cleaned:
            continue
        truncated = cleaned[:MAX_LEN]
        valid_len = len(truncated)
        pad_len = MAX_LEN - valid_len
        seq = [ord(ch) for ch in truncated] + [PAD_TOKEN] * pad_len
        masked_tokens, masked_labels = mask_sequence(seq, valid_len, rng)
        for i in range(valid_len, MAX_LEN):
            masked_labels[i] = -100
        mask = [1] * valid_len + [0] * pad_len
        input_ids.append(masked_tokens)
        labels.append(masked_labels)
        attention_masks.append(mask)
    return {
        "input_ids": np.array(input_ids, dtype=np.int32),
        "labels": np.array(labels, dtype=np.int32),
        "attention_mask": np.array(attention_masks, dtype=np.int8),
    }


def main():
    import time
    start_time = time.time()

    parser = argparse.ArgumentParser()
    parser.add_argument("--fragments", type=Path, required=True)
    parser.add_argument("--out_dir", type=Path, required=True)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    print(f"ðŸ“Š Starting dataset preprocessing...")
    print(f"   Input file: {args.fragments}")
    print(f"   Output dir: {args.out_dir}")
    print(f"   Seed: {args.seed}")
    print(f"   Max sequence length: {MAX_LEN}")
    print(f"   Mask ratio: {MASK_RATIO}")
    print(f"   Span probability: {SPAN_PROB}")

    rng = random.Random(args.seed)
    load_start = time.time()
    with args.fragments.open("r", encoding="utf-8") as f:
        fragments = json.load(f)
    load_time = time.time() - load_start
    print(f"   Loaded {len(fragments)} fragments in {load_time:.2f}s")

    process_start = time.time()
    processed = process_fragments(fragments, rng)
    process_time = time.time() - process_start
    print(f"   Processed {len(processed['input_ids'])} valid sequences in {process_time:.2f}s")

    dataset = Dataset.from_dict(processed)
    dataset = dataset.shuffle(seed=args.seed)

    split_start = time.time()
    ds = dataset.train_test_split(test_size=0.2, seed=args.seed)
    test_val = ds["test"].train_test_split(test_size=0.5, seed=args.seed)
    dataset_dict = DatasetDict({
        "train": ds["train"],
        "validation": test_val["train"],
        "test": test_val["test"],
    })
    split_time = time.time() - split_start

    train_size = len(dataset_dict["train"])
    val_size = len(dataset_dict["validation"])
    test_size = len(dataset_dict["test"])
    print(f"   Dataset splits: Train={train_size}, Val={val_size}, Test={test_size} (split time: {split_time:.2f}s)")

    save_start = time.time()
    args.out_dir.mkdir(parents=True, exist_ok=True)
    dataset_dict.save_to_disk(str(args.out_dir))
    save_time = time.time() - save_start

    total_time = time.time() - start_time
    print(f"âœ… Dataset preprocessing complete in {total_time:.2f}s")
    print(f"   Saved to {args.out_dir} (save time: {save_time:.2f}s)")
    print(f"   Real data verification: Using eBL fragments, not synthetic examples")
    print(f"   Masking applied: {SPAN_PROB*100}% spans, rest single chars")


if __name__ == "__main__":
    main()
